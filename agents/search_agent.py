"""
æœç´¢ Agent (SearchAgent)
è´Ÿè´£ä» arXiv API å¼‚æ­¥è·å–è®ºæ–‡ï¼Œä¼˜åŒ–é€Ÿåº¦
"""
import asyncio
import httpx
import xml.etree.ElementTree as ET
from typing import List, Dict, Any, Optional
from datetime import datetime
from database import db
from config import config


class SearchAgent:
    """arXiv æœç´¢ Agentï¼Œä½¿ç”¨å¼‚æ­¥å¹¶å‘æé«˜è·å–é€Ÿåº¦"""
    
    ARXIV_API_URL = "https://export.arxiv.org/api/query"
    
    def __init__(self):
        """åˆå§‹åŒ–æœç´¢ Agent"""
        self.max_results = config.arxiv_max_results
    
    async def fetch_papers_by_query(
        self, 
        query: str, 
        max_results: int = None
    ) -> List[Dict[str, Any]]:
        """
        å¼‚æ­¥ä» arXiv API è·å–è®ºæ–‡
        
        Args:
            query: arXiv æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
            max_results: æœ€å¤§ç»“æœæ•°é‡
        
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        if max_results is None:
            max_results = self.max_results
        
        params = {
            'search_query': query,
            'start': 0,
            'max_results': max_results,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(self.ARXIV_API_URL, params=params)
                response.raise_for_status()
                
                # è§£æ XML å“åº”
                papers = self._parse_arxiv_response(response.text)
                print(f"âœ“ æŸ¥è¯¢ '{query}' è·å–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
                return papers
                
        except httpx.TimeoutException:
            print(f"âœ— æŸ¥è¯¢ '{query}' è¶…æ—¶")
            return []
        except httpx.HTTPError as e:
            print(f"âœ— æŸ¥è¯¢ '{query}' HTTP é”™è¯¯: {e}")
            return []
        except Exception as e:
            print(f"âœ— æŸ¥è¯¢ '{query}' å¤±è´¥: {e}")
            return []
    
    def _parse_arxiv_response(self, xml_text: str) -> List[Dict[str, Any]]:
        """
        è§£æ arXiv API è¿”å›çš„ XML
        
        Args:
            xml_text: XML å“åº”æ–‡æœ¬
        
        Returns:
            è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        papers = []
        
        try:
            # å®šä¹‰ XML å‘½åç©ºé—´
            namespaces = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            root = ET.fromstring(xml_text)
            
            # éå†æ‰€æœ‰ entry (è®ºæ–‡)
            for entry in root.findall('atom:entry', namespaces):
                try:
                    # æå–è®ºæ–‡ IDï¼ˆä» URL ä¸­æå–ï¼‰
                    id_url = entry.find('atom:id', namespaces).text
                    paper_id = id_url.split('/abs/')[-1]
                    
                    # æå–æ ‡é¢˜ï¼ˆå»é™¤å¤šä½™ç©ºç™½ï¼‰
                    title = entry.find('atom:title', namespaces).text
                    title = ' '.join(title.split())
                    
                    # æå–ä½œè€…åˆ—è¡¨
                    authors = []
                    for author in entry.findall('atom:author', namespaces):
                        name = author.find('atom:name', namespaces)
                        if name is not None:
                            authors.append(name.text)
                    authors_str = ', '.join(authors)
                    
                    # æå–æ‘˜è¦
                    summary = entry.find('atom:summary', namespaces).text
                    summary = ' '.join(summary.split())
                    
                    # æå–åˆ†ç±»
                    categories = []
                    for category in entry.findall('atom:category', namespaces):
                        term = category.get('term')
                        if term:
                            categories.append(term)
                    categories_str = ', '.join(categories)
                    
                    # æå– PDF é“¾æ¥
                    pdf_url = None
                    for link in entry.findall('atom:link', namespaces):
                        if link.get('title') == 'pdf':
                            pdf_url = link.get('href')
                            break
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ° PDF é“¾æ¥ï¼Œæ„é€ é»˜è®¤é“¾æ¥
                    if not pdf_url:
                        pdf_url = f"https://arxiv.org/pdf/{paper_id}.pdf"
                    
                    # æå–å‘å¸ƒæ—¥æœŸ
                    published = entry.find('atom:published', namespaces).text
                    
                    paper = {
                        'id': paper_id,
                        'title': title,
                        'authors': authors_str,
                        'summary': summary,
                        'categories': categories_str,
                        'pdf_url': pdf_url,
                        'published': published
                    }
                    
                    papers.append(paper)
                    
                except Exception as e:
                    print(f"è§£æå•ç¯‡è®ºæ–‡æ—¶å‡ºé”™: {e}")
                    continue
            
        except Exception as e:
            print(f"è§£æ XML å“åº”å¤±è´¥: {e}")
        
        return papers
    
    async def fetch_papers_for_all_topics(self) -> Dict[str, List[Dict[str, Any]]]:
        """
        å¹¶å‘è·å–æ‰€æœ‰ä¸»é¢˜çš„è®ºæ–‡ï¼ˆé€Ÿåº¦ä¼˜åŒ–ï¼‰
        
        Returns:
            {topic_name: [papers]} çš„å­—å…¸
        """
        topics = db.get_topics()
        
        if not topics:
            print("âš  æ²¡æœ‰é…ç½®ä»»ä½•ä¸»é¢˜")
            return {}
        
        # åˆ›å»ºæ‰€æœ‰ä¸»é¢˜çš„å¼‚æ­¥ä»»åŠ¡
        tasks = []
        topic_names = []
        
        for topic in topics:
            task = self.fetch_papers_by_query(topic['query'])
            tasks.append(task)
            topic_names.append(topic['name'])
        
        print(f"ğŸš€ å¼€å§‹å¹¶å‘è·å– {len(tasks)} ä¸ªä¸»é¢˜çš„è®ºæ–‡...")
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ•´ç†ç»“æœ
        topic_papers = {}
        for topic_name, result in zip(topic_names, results):
            if isinstance(result, Exception):
                print(f"âœ— ä¸»é¢˜ '{topic_name}' è·å–å¤±è´¥: {result}")
                topic_papers[topic_name] = []
            else:
                topic_papers[topic_name] = result
        
        return topic_papers
    
    def save_papers_to_db(self, papers: List[Dict[str, Any]]) -> int:
        """
        å°†è®ºæ–‡ä¿å­˜åˆ°æ•°æ®åº“
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
        
        Returns:
            æ–°å¢è®ºæ–‡æ•°é‡
        """
        new_count = 0
        
        for paper in papers:
            if db.add_paper(paper):
                new_count += 1
        
        return new_count
    
    async def fetch_and_save_all(self) -> Dict[str, int]:
        """
        è·å–æ‰€æœ‰ä¸»é¢˜çš„è®ºæ–‡å¹¶ä¿å­˜åˆ°æ•°æ®åº“
        
        Returns:
            {topic_name: new_papers_count} çš„å­—å…¸
        """
        topic_papers = await self.fetch_papers_for_all_topics()
        
        results = {}
        total_new = 0
        
        for topic_name, papers in topic_papers.items():
            new_count = self.save_papers_to_db(papers)
            results[topic_name] = new_count
            total_new += new_count
            
            # æ›´æ–°ä¸»é¢˜çš„æœ€åè·å–æ—¶é—´
            topics = db.get_topics()
            for topic in topics:
                if topic['name'] == topic_name:
                    db.update_topic_last_fetched(topic['id'])
                    break
        
        print(f"âœ“ æ€»å…±æ–°å¢ {total_new} ç¯‡è®ºæ–‡")
        return results
    
    async def search_and_add(self, query: str, max_results: int = 20) -> int:
        """
        æœç´¢å¹¶æ·»åŠ è®ºæ–‡ï¼ˆç”¨äºæ‰‹åŠ¨æœç´¢ï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
        
        Returns:
            æ–°å¢è®ºæ–‡æ•°é‡
        """
        papers = await self.fetch_papers_by_query(query, max_results)
        new_count = self.save_papers_to_db(papers)
        print(f"âœ“ æœç´¢ '{query}' æ–°å¢ {new_count} ç¯‡è®ºæ–‡")
        return new_count


# åˆ›å»ºå…¨å±€å®ä¾‹
search_agent = SearchAgent()
