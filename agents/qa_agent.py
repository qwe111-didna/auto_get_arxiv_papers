"""
é—®ç­” Agent (QAAgent)
å®ç° RAG (Retrieval-Augmented Generation) æ™ºèƒ½é—®ç­”
"""
from typing import List, Dict, Any, Optional
from config import config
from .indexing_agent import indexing_agent


class QAAgent:
    """é—®ç­” Agentï¼ŒåŸºäº RAG æ¶æ„å›ç­”ç”¨æˆ·é—®é¢˜"""
    
    def __init__(self):
        """åˆå§‹åŒ–é—®ç­” Agent"""
        self.client = config.get_client()
        self.model = "Qwen/Qwen2.5-7B-Instruct"  # ä½¿ç”¨è¾ƒå¤§çš„æ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„å›ç­”
        self.indexing_agent = indexing_agent
    
    def answer(
        self, 
        question: str, 
        top_k: int = 5,
        stream: bool = False
    ) -> Dict[str, Any]:
        """
        å›ç­”ç”¨æˆ·é—®é¢˜ï¼ˆRAG æµç¨‹ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢çš„è®ºæ–‡æ•°é‡
            stream: æ˜¯å¦æµå¼è¿”å›
        
        Returns:
            åŒ…å«ç­”æ¡ˆå’Œå¼•ç”¨æ¥æºçš„å­—å…¸
        """
        if not self.client:
            return {
                'answer': 'âŒ é—®ç­”æœåŠ¡ä¸å¯ç”¨ï¼Œè¯·é…ç½® MS_API_KEY',
                'sources': [],
                'error': 'LLM service unavailable'
            }
        
        if not self.indexing_agent.is_available():
            return {
                'answer': 'âŒ æ£€ç´¢æœåŠ¡ä¸å¯ç”¨ï¼Œè¯·å…ˆå»ºç«‹è®ºæ–‡ç´¢å¼•',
                'sources': [],
                'error': 'Indexing service unavailable'
            }
        
        try:
            # 1. æ£€ç´¢ (Retrieve) - æ‰¾åˆ°ç›¸å…³è®ºæ–‡
            print(f"ğŸ” æ£€ç´¢ä¸é—®é¢˜ç›¸å…³çš„è®ºæ–‡...")
            relevant_papers = self.indexing_agent.search(question, top_k=top_k)
            
            if not relevant_papers:
                return {
                    'answer': 'æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„è®ºæ–‡ã€‚è¯·å°è¯•æ¢ä¸ªæ–¹å¼æé—®ï¼Œæˆ–è€…å…ˆæ·»åŠ ä¸€äº›ç›¸å…³ä¸»é¢˜çš„è®ºæ–‡ã€‚',
                    'sources': [],
                    'error': 'No relevant papers found'
                }
            
            # 2. å¢å¼º (Augment) - æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context(relevant_papers)
            
            # 3. ç”Ÿæˆ (Generate) - è®© LLM å›ç­”
            print(f"ğŸ¤– æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
            
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘ç ”åŠ©ç†ï¼Œæ“…é•¿é˜…è¯»å’Œç†è§£å­¦æœ¯è®ºæ–‡ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„è®ºæ–‡æ‘˜è¦ï¼Œå‡†ç¡®ã€æ¸…æ™°åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å›ç­”è¦æ±‚ï¼š
1. åŸºäºæä¾›çš„è®ºæ–‡å†…å®¹å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœè®ºæ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. ä½¿ç”¨ä¸­æ–‡å›ç­”
4. å›ç­”è¦ä¸“ä¸šä½†æ˜“æ‡‚
5. é€‚å½“å¼•ç”¨è®ºæ–‡å†…å®¹æ”¯æŒä½ çš„è§‚ç‚¹"""

            user_prompt = f"""åŸºäºä»¥ä¸‹å­¦æœ¯è®ºæ–‡æ‘˜è¦ï¼Œè¯·å›ç­”é—®é¢˜ã€‚

è®ºæ–‡æ‘˜è¦ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†çš„å›ç­”ï¼š"""

            if stream:
                # æµå¼è¿”å›ï¼ˆç”¨äºå®æ—¶æ˜¾ç¤ºï¼‰
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {'role': 'system', 'content': system_prompt},
                        {'role': 'user', 'content': user_prompt}
                    ],
                    stream=True,
                    temperature=0.7,
                    max_tokens=2000
                )
                
                return {
                    'stream': response,
                    'sources': self._format_sources(relevant_papers)
                }
            else:
                # éæµå¼è¿”å›
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {'role': 'system', 'content': system_prompt},
                        {'role': 'user', 'content': user_prompt}
                    ],
                    temperature=0.7,
                    max_tokens=2000
                )
                
                answer = response.choices[0].message.content
                
                return {
                    'answer': answer,
                    'sources': self._format_sources(relevant_papers)
                }
        
        except Exception as e:
            print(f"é—®ç­”å¤±è´¥: {e}")
            return {
                'answer': f'âŒ ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}',
                'sources': [],
                'error': str(e)
            }
    
    def _build_context(self, papers: List[Dict[str, Any]]) -> str:
        """
        æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        
        Args:
            papers: æ£€ç´¢åˆ°çš„è®ºæ–‡åˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        context_parts = []
        
        for i, paper in enumerate(papers, 1):
            metadata = paper['metadata']
            document = paper['document']
            
            # æˆªå–æ–‡æ¡£ï¼ˆé¿å…å¤ªé•¿ï¼‰
            if len(document) > 1500:
                document = document[:1500] + "..."
            
            context_part = f"""[è®ºæ–‡ {i}]
æ ‡é¢˜: {metadata['title']}
ä½œè€…: {metadata['authors']}
åˆ†ç±»: {metadata['categories']}
å†…å®¹: {document}
"""
            context_parts.append(context_part)
        
        return "\n\n".join(context_parts)
    
    def _format_sources(self, papers: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æ ¼å¼åŒ–å¼•ç”¨æ¥æº
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–çš„æ¥æºåˆ—è¡¨
        """
        sources = []
        
        for paper in papers:
            metadata = paper['metadata']
            source = {
                'id': paper['id'],
                'title': metadata['title'],
                'authors': metadata['authors'],
                'pdf_url': metadata['pdf_url'],
                'published': metadata['published'],
                'relevance': f"{(1 - paper['distance']) * 100:.1f}%" if paper.get('distance') else 'N/A'
            }
            sources.append(source)
        
        return sources
    
    def answer_stream(self, question: str, top_k: int = 5):
        """
        æµå¼å›ç­”ï¼ˆç”Ÿæˆå™¨ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢çš„è®ºæ–‡æ•°é‡
        
        Yields:
            ç­”æ¡ˆç‰‡æ®µæˆ–å®Œæ•´çš„æ¥æºä¿¡æ¯
        """
        if not self.client or not self.indexing_agent.is_available():
            yield {
                'type': 'error',
                'content': 'æœåŠ¡ä¸å¯ç”¨'
            }
            return
        
        try:
            # æ£€ç´¢ç›¸å…³è®ºæ–‡
            relevant_papers = self.indexing_agent.search(question, top_k=top_k)
            
            if not relevant_papers:
                yield {
                    'type': 'error',
                    'content': 'æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡'
                }
                return
            
            # å…ˆå‘é€æ¥æºä¿¡æ¯
            yield {
                'type': 'sources',
                'content': self._format_sources(relevant_papers)
            }
            
            # æ„å»ºä¸Šä¸‹æ–‡å¹¶ç”Ÿæˆç­”æ¡ˆ
            context = self._build_context(relevant_papers)
            
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘ç ”åŠ©ç†ï¼Œæ“…é•¿é˜…è¯»å’Œç†è§£å­¦æœ¯è®ºæ–‡ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„è®ºæ–‡æ‘˜è¦ï¼Œå‡†ç¡®ã€æ¸…æ™°åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å›ç­”è¦æ±‚ï¼š
1. åŸºäºæä¾›çš„è®ºæ–‡å†…å®¹å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœè®ºæ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. ä½¿ç”¨ä¸­æ–‡å›ç­”
4. å›ç­”è¦ä¸“ä¸šä½†æ˜“æ‡‚
5. é€‚å½“å¼•ç”¨è®ºæ–‡å†…å®¹æ”¯æŒä½ çš„è§‚ç‚¹"""

            user_prompt = f"""åŸºäºä»¥ä¸‹å­¦æœ¯è®ºæ–‡æ‘˜è¦ï¼Œè¯·å›ç­”é—®é¢˜ã€‚

è®ºæ–‡æ‘˜è¦ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†çš„å›ç­”ï¼š"""

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {'role': 'system', 'content': system_prompt},
                    {'role': 'user', 'content': user_prompt}
                ],
                stream=True,
                temperature=0.7,
                max_tokens=2000
            )
            
            # æµå¼è¿”å›ç­”æ¡ˆ
            for chunk in response:
                content = chunk.choices[0].delta.content
                if content:
                    yield {
                        'type': 'answer',
                        'content': content
                    }
        
        except Exception as e:
            yield {
                'type': 'error',
                'content': str(e)
            }


# åˆ›å»ºå…¨å±€å®ä¾‹
qa_agent = QAAgent()
